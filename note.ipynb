{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.environments.cubby_environment import CubbyEnvironment \n",
    "from robofin.collision import FrankaSelfCollisionChecker\n",
    "\n",
    "env = CubbyEnvironment()\n",
    "selfcc = FrankaSelfCollisionChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(qs_free): 50\n",
      "len(qs_collision): 429\n"
     ]
    }
   ],
   "source": [
    "obstacles, qs_free, poses_free, qs_collision, poses_collision = env.sample_q_pose(selfcc=selfcc, how_many=50, margin=0)\n",
    "print(\"len(qs_free):\", len(qs_free))\n",
    "print(\"len(qs_collision):\", len(qs_collision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robot_points: torch.Size([1, 2048, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from robofin.pointcloud.torch import FrankaSampler\n",
    "fk_sampler = FrankaSampler(\"cpu\", use_cache=True)\n",
    "\n",
    "qs = qs_collision\n",
    "q = torch.tensor(qs[5], dtype=torch.float32, requires_grad=False)\n",
    "robot_points = fk_sampler.sample(q, 2048)\n",
    "print(\"robot_points:\", robot_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.geometry import construct_mixed_point_cloud\n",
    "obstacle_points = construct_mixed_point_cloud(obstacles, num_points=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Create an Open3D PointCloud object for the robot points\n",
    "robot_pcd = o3d.geometry.PointCloud()\n",
    "robot_points = robot_points.squeeze().cpu().numpy()  # Convert to numpy array\n",
    "robot_points = robot_points.reshape(-1, 3)  # Ensure correct shape\n",
    "robot_pcd.points = o3d.utility.Vector3dVector(robot_points)  # Use precomputed robot_points\n",
    "robot_pcd.paint_uniform_color([1, 0, 0])  # Red for robot points\n",
    "\n",
    "# Create an Open3D PointCloud object for the obstacle points\n",
    "obstacle_pcd = o3d.geometry.PointCloud()\n",
    "obstacle_pcd.points = o3d.utility.Vector3dVector(obstacle_points[:, :3])  # Use precomputed obstacle_points\n",
    "obstacle_pcd.paint_uniform_color([0, 1, 0])  # Green for obstacle points\n",
    "\n",
    "# Combine the two point clouds into a single list\n",
    "pcd = [robot_pcd, obstacle_pcd]\n",
    "\n",
    "# Visualize both point clouds together\n",
    "o3d.visualization.draw_geometries(pcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robot_points min: [-0.15131235 -0.10666256  0.        ]\n",
      "robot_points max: [0.79742277 0.44996527 0.5832137 ]\n",
      "obstacle_points min: [ 5.22638759e-01 -6.67001633e-01  5.55111512e-17]\n",
      "obstacle_points max: [1.22289878 0.82198416 0.73705289]\n"
     ]
    }
   ],
   "source": [
    "# Check the boundaries of the point clouds\n",
    "robot_points = robot_points.reshape(-1, 3)  # Ensure correct shape\n",
    "print(\"robot_points min:\", robot_points.min(axis=0))\n",
    "print(\"robot_points max:\", robot_points.max(axis=0))\n",
    "obstacle_points = obstacle_points[:, :3]\n",
    "print(\"obstacle_points min:\", obstacle_points.min(axis=0))\n",
    "print(\"obstacle_points max:\", obstacle_points.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch  # For loading and processing tensor data\n",
    "import open3d as o3d  # For 3D visualization\n",
    "import matplotlib.pyplot as plt  # For plotting and visualization in 2D\n",
    "\n",
    "# Ensure matplotlib inline mode for Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_272683/2099462957.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  samples = torch.load(tensor_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset loaded from ./collision_bool/processed.pt\n",
      "Number of samples loaded: 50000\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "\n",
    "# Define the path to the tensor file\n",
    "tensor_file = \"./collision_bool/processed.pt\"\n",
    "\n",
    "# Load the dataset using the provided function\n",
    "def load_full_data_tensor(tensor_file):\n",
    "    \"\"\"\n",
    "    Load the full dataset from a tensor file.\n",
    "    \n",
    "    Args:\n",
    "        tensor_file (str): Path to the tensor file\n",
    "        \n",
    "    Returns:\n",
    "        List of samples\n",
    "    \"\"\"\n",
    "    samples = torch.load(tensor_file)\n",
    "    print(f\"Full dataset loaded from {tensor_file}\")\n",
    "    return samples\n",
    "\n",
    "# Load the dataset\n",
    "dataset_samples = load_full_data_tensor(tensor_file)\n",
    "\n",
    "# Display the number of samples loaded\n",
    "print(f\"Number of samples loaded: {len(dataset_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot Points Min: [-0.24984953 -0.834266    0.        ]\n",
      "Robot Points Max: [0.9602809 0.8245288 0.8998966]\n",
      "Obstacle Points Min: [ 3.2435596e-01 -8.4170550e-01 -1.6653345e-16]\n",
      "Obstacle Points Max: [1.3019081  0.84264076 0.81443304]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Iterate through the dataset samples to check the boundaries of point clouds\n",
    "robot_min, robot_max = [], []\n",
    "obstacle_min, obstacle_max = [], []\n",
    "\n",
    "for sample in dataset_samples:\n",
    "    pointcloud_data = sample['pointcloud'].numpy()\n",
    "    robot_points = pointcloud_data[pointcloud_data[:, 3] == 0][:, :3]\n",
    "    obstacle_points = pointcloud_data[pointcloud_data[:, 3] == 1][:, :3]\n",
    "    \n",
    "    robot_min.append(robot_points.min(axis=0))\n",
    "    robot_max.append(robot_points.max(axis=0))\n",
    "    obstacle_min.append(obstacle_points.min(axis=0))\n",
    "    obstacle_max.append(obstacle_points.max(axis=0))\n",
    "\n",
    "# Compute overall min and max for robot and obstacle points\n",
    "robot_min = np.min(robot_min, axis=0)\n",
    "robot_max = np.max(robot_max, axis=0)\n",
    "obstacle_min = np.min(obstacle_min, axis=0)\n",
    "obstacle_max = np.max(obstacle_max, axis=0)\n",
    "\n",
    "print(\"Robot Points Min:\", robot_min)\n",
    "print(\"Robot Points Max:\", robot_max)\n",
    "print(\"Obstacle Points Min:\", obstacle_min)\n",
    "print(\"Obstacle Points Max:\", obstacle_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     obstacle_max\u001b[38;5;241m.\u001b[39mappend(obstacle_points\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute overall min and max for robot and obstacle points\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m robot_min \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mmin(robot_min, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m robot_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(robot_max, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m obstacle_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(obstacle_min, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize a Sample Point Cloud\n",
    "\n",
    "# Select a sample from the dataset\n",
    "sample_index = 100  # Change this index to visualize a different sample\n",
    "sample = dataset_samples[sample_index]\n",
    "\n",
    "# Extract the point cloud data\n",
    "pointcloud_data = sample['pointcloud'].numpy()\n",
    "\n",
    "print(\"Collison:\", sample['collision_flag'])\n",
    "\n",
    "# The first dim is the feature dimension, the rest are the point coordinates\n",
    "# 0 for robot points, 1 for obstacle points\n",
    "\n",
    "# Separate robot and obstacle points based on the feature dimension\n",
    "robot_points = pointcloud_data[pointcloud_data[:, 3] == 0][:, :3]\n",
    "obstacle_points = pointcloud_data[pointcloud_data[:, 3] == 1][:, :3]\n",
    "\n",
    "# Create Open3D point clouds\n",
    "robot_pcd = o3d.geometry.PointCloud()\n",
    "robot_pcd.points = o3d.utility.Vector3dVector(robot_points)\n",
    "robot_pcd.paint_uniform_color([1, 0, 0])  # Red for robot points\n",
    "\n",
    "obstacle_pcd = o3d.geometry.PointCloud()\n",
    "obstacle_pcd.points = o3d.utility.Vector3dVector(obstacle_points)\n",
    "obstacle_pcd.paint_uniform_color([0, 1, 0])  # Green for obstacle points\n",
    "\n",
    "# Visualize both point clouds\n",
    "o3d.visualization.draw_geometries([robot_pcd, obstacle_pcd])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20049/1139805233.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  samples = torch.load(tensor_file)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.ptv3 import PointTransformerV3\n",
    "\n",
    "# Load the processed dataset from the tensor file\n",
    "tensor_file = \"./collision_bool/processed.pt\"  # Path to your processed tensor file\n",
    "samples = torch.load(tensor_file)\n",
    "\n",
    "# Initialize the PointTransformerV3 model\n",
    "model = PointTransformerV3(\n",
    "    in_channels=4,  # Number of input features per point (e.g., x, y, z, feature flag)\n",
    "    enc_depths=(2, 2, 2, 6, 2),\n",
    "    enc_channels=(32, 64, 128, 256, 512),\n",
    "    enc_num_head=(2, 4, 8, 16, 32),\n",
    "    enc_patch_size=(1024, 1024, 1024, 1024, 1024),\n",
    "    cls_mode=True  # Set to True if you only want the encoded features\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare a batch of samples from the loaded dataset\n",
    "batch_size = 1  # Number of samples in the batch\n",
    "batch_samples = samples[:batch_size]\n",
    "\n",
    "# Prepare the input dictionary\n",
    "pointclouds = torch.cat([sample[\"pointcloud\"] for sample in batch_samples], dim=0).to(device)  # Concatenate point clouds\n",
    "batch_indices = torch.cat([\n",
    "    torch.zeros(len(sample[\"pointcloud\"][:2048]), dtype=torch.long) for sample in batch_samples  # Robot points\n",
    "] + [\n",
    "    torch.ones(len(sample[\"pointcloud\"][2048:]), dtype=torch.long) for sample in batch_samples  # Obstacle points\n",
    "]).to(device)\n",
    "\n",
    "input_data = {\n",
    "    \"coord\": pointclouds[:, :3],  # Extract x, y, z coordinates\n",
    "    \"feat\": pointclouds,         # Use the full point cloud as features (x, y, z, feature flag)\n",
    "    \"batch\": batch_indices,      # Batch indices (0 for robot points, 1 for obstacle points)\n",
    "    \"grid_size\": torch.tensor(0.02).to(device)  # Grid size for voxelization\n",
    "}\n",
    "\n",
    "# Forward pass through the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    encoded_features = model(input_data)\n",
    "    # Perform global max pooling to aggregate features into a single vector\n",
    "    global_feature_vector = torch.max(encoded_features[\"feat\"], dim=0).values\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Features keys: dict_keys(['feat', 'coord', 'grid_coord', 'serialized_code', 'serialized_order', 'serialized_inverse', 'serialized_depth', 'batch', 'pooling_inverse', 'pooling_parent', 'offset', 'sparse_shape', 'sparse_conv_feat', 'pad', 'unpad', 'cu_seqlens_key'])\n",
      "Encoded Features Shape: torch.Size([41, 512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# Output the encoded features\n",
    "print(\"Encoded Features keys:\", encoded_features.keys())\n",
    "print(\"Encoded Features Shape:\", encoded_features[\"feat\"].shape)\n",
    "print(global_feature_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62848/552146641.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  samples = torch.load(tensor_file)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.ptv3 import PointTransformerNet\n",
    "# Load the processed dataset from the tensor file\n",
    "tensor_file = \"./collision_bool/processed.pt\"  # Path to your processed tensor file\n",
    "samples = torch.load(tensor_file)\n",
    "# Prepare a batch of samples from the loaded dataset\n",
    "batch_size = 8  # Define the batch size\n",
    "batch_samples = samples[:batch_size]  # Select the first 'batch_size' samples\n",
    "\n",
    "# Concatenate point clouds from all samples in the batch\n",
    "pointcloud = torch.cat([sample[\"pointcloud\"] for sample in batch_samples], dim=0)\n",
    "\n",
    "# Initialize the PointTransformerV3 model\n",
    "model = PointTransformerNet()\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "pointcloud = pointcloud.to(device)  # Move the point cloud to the same device as the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6144) must match the size of tensor b (49152) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpointcloud\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass the point cloud through the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, feature\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Print the shape of the output feature\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/pointnet2/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pointnet2/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/collision-net/models/ptv3.py:1030\u001b[0m, in \u001b[0;36mPointTransformerNet.forward\u001b[0;34m(self, point_cloud)\u001b[0m\n\u001b[1;32m   1022\u001b[0m input_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoord\u001b[39m\u001b[38;5;124m\"\u001b[39m: point_cloud[:, :\u001b[38;5;241m3\u001b[39m],  \u001b[38;5;66;03m# Extract x, y, z coordinates\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeat\u001b[39m\u001b[38;5;124m\"\u001b[39m: point_cloud,         \u001b[38;5;66;03m# Use the full point cloud as features (x, y, z, feature flag)\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_indices,  \u001b[38;5;66;03m# Batch indices\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.02\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Grid size for voxelization\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m }\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;66;03m# Forward pass through PointTransformerV3\u001b[39;00m\n\u001b[0;32m-> 1030\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# Global max pooling to get single feature vector\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m global_feature \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output\u001b[38;5;241m.\u001b[39mfeat, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/pointnet2/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pointnet2/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/collision-net/models/ptv3.py:975\u001b[0m, in \u001b[0;36mPointTransformerV3.forward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;124;03mA data_dict is a dictionary containing properties of a batched point cloud.\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;124;03mIt should contain the following properties for PTv3:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m3. \"offset\" or \"batch\": https://github.com/Pointcept/Pointcept?tab=readme-ov-file#offset\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    974\u001b[0m point \u001b[38;5;241m=\u001b[39m Point(data_dict)\n\u001b[0;32m--> 975\u001b[0m \u001b[43mpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_orders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle_orders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m point\u001b[38;5;241m.\u001b[39msparsify()\n\u001b[1;32m    978\u001b[0m point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(point)\n",
      "File \u001b[0;32m~/Programs/collision-net/models/ptv3.py:115\u001b[0m, in \u001b[0;36mPoint.serialization\u001b[0;34m(self, order, depth, shuffle_orders)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m depth \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# The serialization codes are arranged as following structures:\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# [Order1 ([n]),\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m#  Order2 ([n]),\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m#   ...\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#  OrderN ([n])] (k, n)\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m code \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    116\u001b[0m     encode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_coord, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch, depth, order\u001b[38;5;241m=\u001b[39morder_) \u001b[38;5;28;01mfor\u001b[39;00m order_ \u001b[38;5;129;01min\u001b[39;00m order\n\u001b[1;32m    117\u001b[0m ]\n\u001b[1;32m    118\u001b[0m code \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(code)\n\u001b[1;32m    119\u001b[0m order \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(code)\n",
      "File \u001b[0;32m~/Programs/collision-net/models/ptv3.py:116\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m depth \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# The serialization codes are arranged as following structures:\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# [Order1 ([n]),\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m#  Order2 ([n]),\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m#   ...\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#  OrderN ([n])] (k, n)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m code \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 116\u001b[0m     \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_coord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m order_ \u001b[38;5;129;01min\u001b[39;00m order\n\u001b[1;32m    117\u001b[0m ]\n\u001b[1;32m    118\u001b[0m code \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(code)\n\u001b[1;32m    119\u001b[0m order \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(code)\n",
      "File \u001b[0;32m~/mambaforge/envs/pointnet2/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/collision-net/models/serialization/default.py:23\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(grid_coord, batch, depth, order)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m---> 23\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m code\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6144) must match the size of tensor b (49152) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    feature = model(pointcloud)  # Pass the point cloud through the model\n",
    "    \n",
    "print(\"Feature shape:\", feature.shape)  # Print the shape of the output feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/zippingsugar/Programs/collision-net/data_loader.py:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  samples = torch.load(tensor_file)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset loaded from ./collision_bool/processed_test.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3353528c19474bbfc5d57cb060a6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9404040575027466\n",
      "        test_loss           0.1574736088514328\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Test results: [{'test_loss': 0.1574736088514328, 'test_acc': 0.9404040575027466}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from train import CollisionNetPL\n",
    "from data_loader import get_data_loaders_from_tensor\n",
    "\n",
    "# Path to the trained model checkpoint\n",
    "checkpoint_path = 'checkpoints_pointnet/collisionnet-epoch=58-val_loss=0.14.ckpt' \n",
    "# checkpoint_path = 'checkpoints_pointnet2/collisionnet-epoch=58-val_loss=0.28.ckpt'\n",
    "\n",
    "# Load the trained model\n",
    "model = CollisionNetPL.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
    "\n",
    "# Load the test dataset\n",
    "_, test_loader = get_data_loaders_from_tensor(\n",
    "    \"./collision_bool/processed_test.pt\",  # Update with the actual test dataset path\n",
    "    batch_size=64,\n",
    "    train_ratio=0.01  # No training split, purely test data\n",
    ")\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "# Run the test\n",
    "results = trainer.test(model, dataloaders=test_loader)\n",
    "\n",
    "# Print the test results\n",
    "print(f\"Test results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CollisionNet(\n",
      "  (point_cloud_encoder): PointNet2(\n",
      "    (SA_modules): ModuleList(\n",
      "      (0): PointnetSAModule(\n",
      "        (groupers): ModuleList(\n",
      "          (0): QueryAndGroup()\n",
      "        )\n",
      "        (mlps): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): PointnetSAModule(\n",
      "        (groupers): ModuleList(\n",
      "          (0): QueryAndGroup()\n",
      "        )\n",
      "        (mlps): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(67, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (4): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PointnetSAModule(\n",
      "        (groupers): ModuleList(\n",
      "          (0): GroupAll()\n",
      "        )\n",
      "        (mlps): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(259, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (4): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc_layer): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (1): GroupNorm(16, 4096, eps=1e-05, affine=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (3): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "      (4): GroupNorm(16, 2048, eps=1e-05, affine=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (feature_encoder): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=32, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=2112, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.collisionnet import CollisionNet\n",
    "# Initialize the CollisionNet model\n",
    "collision_net = CollisionNet()\n",
    "\n",
    "# Print the structure of the CollisionNet model\n",
    "print(collision_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointnet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
