{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:19:57\n"
     ]
    }
   ],
   "source": [
    "from data_pipeline.environments.cubby_environment import CubbyEnvironment \n",
    "from robofin.collision import FrankaSelfCollisionChecker\n",
    "\n",
    "env = CubbyEnvironment()\n",
    "selfcc = FrankaSelfCollisionChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(qs_free): 50\n",
      "len(qs_collision): 545\n"
     ]
    }
   ],
   "source": [
    "obstacles, qs_free, poses_free, qs_collision, poses_collision = env.sample_q_pose(selfcc=selfcc, how_many=50, margin=0)\n",
    "print(\"len(qs_free):\", len(qs_free))\n",
    "print(\"len(qs_collision):\", len(qs_collision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robot_points: torch.Size([1, 2048, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from robofin.pointcloud.torch import FrankaSampler\n",
    "fk_sampler = FrankaSampler(\"cpu\", use_cache=True)\n",
    "\n",
    "qs = qs_collision\n",
    "q = torch.tensor(qs[5], dtype=torch.float32, requires_grad=False)\n",
    "robot_points = fk_sampler.sample(q, 2048)\n",
    "print(\"robot_points:\", robot_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.geometry import construct_mixed_point_cloud\n",
    "obstacle_points = construct_mixed_point_cloud(obstacles, num_points=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Create an Open3D PointCloud object for the robot points\n",
    "robot_pcd = o3d.geometry.PointCloud()\n",
    "robot_points = robot_points.squeeze().cpu().numpy()  # Convert to numpy array\n",
    "robot_points = robot_points.reshape(-1, 3)  # Ensure correct shape\n",
    "robot_pcd.points = o3d.utility.Vector3dVector(robot_points)  # Use precomputed robot_points\n",
    "robot_pcd.paint_uniform_color([1, 0, 0])  # Red for robot points\n",
    "\n",
    "# Create an Open3D PointCloud object for the obstacle points\n",
    "obstacle_pcd = o3d.geometry.PointCloud()\n",
    "obstacle_pcd.points = o3d.utility.Vector3dVector(obstacle_points[:, :3])  # Use precomputed obstacle_points\n",
    "obstacle_pcd.paint_uniform_color([0, 1, 0])  # Green for obstacle points\n",
    "\n",
    "# Combine the two point clouds into a single list\n",
    "pcd = [robot_pcd, obstacle_pcd]\n",
    "\n",
    "# Visualize both point clouds together\n",
    "o3d.visualization.draw_geometries(pcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch  # For loading and processing tensor data\n",
    "import open3d as o3d  # For 3D visualization\n",
    "import matplotlib.pyplot as plt  # For plotting and visualization in 2D\n",
    "\n",
    "# Ensure matplotlib inline mode for Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104855/2099462957.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  samples = torch.load(tensor_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset loaded from ./collision_bool/processed.pt\n",
      "Number of samples loaded: 50000\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "\n",
    "# Define the path to the tensor file\n",
    "tensor_file = \"./collision_bool/processed.pt\"\n",
    "\n",
    "# Load the dataset using the provided function\n",
    "def load_full_data_tensor(tensor_file):\n",
    "    \"\"\"\n",
    "    Load the full dataset from a tensor file.\n",
    "    \n",
    "    Args:\n",
    "        tensor_file (str): Path to the tensor file\n",
    "        \n",
    "    Returns:\n",
    "        List of samples\n",
    "    \"\"\"\n",
    "    samples = torch.load(tensor_file)\n",
    "    print(f\"Full dataset loaded from {tensor_file}\")\n",
    "    return samples\n",
    "\n",
    "# Load the dataset\n",
    "dataset_samples = load_full_data_tensor(tensor_file)\n",
    "\n",
    "# Display the number of samples loaded\n",
    "print(f\"Number of samples loaded: {len(dataset_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collison: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Visualize a Sample Point Cloud\n",
    "\n",
    "# Select a sample from the dataset\n",
    "sample_index = 0  # Change this index to visualize a different sample\n",
    "sample = dataset_samples[sample_index]\n",
    "\n",
    "# Extract the point cloud data\n",
    "pointcloud_data = sample['pointcloud'].numpy()\n",
    "\n",
    "print(\"Collison:\", sample['collision_flag'])\n",
    "\n",
    "# The first dim is the feature dimension, the rest are the point coordinates\n",
    "# 0 for robot points, 1 for obstacle points\n",
    "\n",
    "# Separate robot and obstacle points based on the feature dimension\n",
    "robot_points = pointcloud_data[pointcloud_data[:, 3] == 0][:, :3]\n",
    "obstacle_points = pointcloud_data[pointcloud_data[:, 3] == 1][:, :3]\n",
    "\n",
    "# Create Open3D point clouds\n",
    "robot_pcd = o3d.geometry.PointCloud()\n",
    "robot_pcd.points = o3d.utility.Vector3dVector(robot_points)\n",
    "robot_pcd.paint_uniform_color([1, 0, 0])  # Red for robot points\n",
    "\n",
    "obstacle_pcd = o3d.geometry.PointCloud()\n",
    "obstacle_pcd.points = o3d.utility.Vector3dVector(obstacle_points)\n",
    "obstacle_pcd.paint_uniform_color([0, 1, 0])  # Green for obstacle points\n",
    "\n",
    "# Visualize both point clouds\n",
    "o3d.visualization.draw_geometries([robot_pcd, obstacle_pcd])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/zippingsugar/mambaforge/envs/pointnet2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/tmp/ipykernel_111691/1715093271.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  samples = torch.load(tensor_file)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.ptv3 import PointTransformerV3\n",
    "\n",
    "# Load the processed dataset from the tensor file\n",
    "tensor_file = \"./collision_bool/processed.pt\"  # Path to your processed tensor file\n",
    "samples = torch.load(tensor_file)\n",
    "\n",
    "# Initialize the PointTransformerV3 model\n",
    "model = PointTransformerV3(\n",
    "    in_channels=4,  # Number of input features per point (e.g., x, y, z, feature flag)\n",
    "    enc_depths=(2, 2, 2, 6, 2),\n",
    "    enc_channels=(32, 64, 128, 256, 512),\n",
    "    enc_num_head=(2, 4, 8, 16, 32),\n",
    "    enc_patch_size=(1024, 1024, 1024, 1024, 1024),\n",
    "    cls_mode=True  # Set to True if you only want the encoded features\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare a batch of samples from the loaded dataset\n",
    "batch_size = 2  # Number of samples in the batch\n",
    "batch_samples = samples[:batch_size]\n",
    "\n",
    "# Prepare the input dictionary\n",
    "pointclouds = torch.cat([sample[\"pointcloud\"] for sample in batch_samples], dim=0).to(device)  # Concatenate point clouds\n",
    "batch_indices = torch.cat([\n",
    "    torch.zeros(len(sample[\"pointcloud\"][:2048]), dtype=torch.long) for sample in batch_samples  # Robot points\n",
    "] + [\n",
    "    torch.ones(len(sample[\"pointcloud\"][2048:]), dtype=torch.long) for sample in batch_samples  # Obstacle points\n",
    "]).to(device)\n",
    "\n",
    "input_data = {\n",
    "    \"coord\": pointclouds[:, :3],  # Extract x, y, z coordinates\n",
    "    \"feat\": pointclouds,         # Use the full point cloud as features (x, y, z, feature flag)\n",
    "    \"batch\": batch_indices,      # Batch indices (0 for robot points, 1 for obstacle points)\n",
    "    \"grid_size\": torch.tensor(0.02).to(device)  # Grid size for voxelization\n",
    "}\n",
    "\n",
    "# Forward pass through the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    encoded_features = model(input_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Features keys: dict_keys(['feat', 'coord', 'grid_coord', 'serialized_code', 'serialized_order', 'serialized_inverse', 'serialized_depth', 'batch', 'pooling_inverse', 'pooling_parent', 'offset', 'sparse_shape', 'sparse_conv_feat', 'pad', 'unpad', 'cu_seqlens_key'])\n",
      "Encoded Features Shape: torch.Size([93, 512])\n"
     ]
    }
   ],
   "source": [
    "# Output the encoded features\n",
    "print(\"Encoded Features keys:\", encoded_features.keys())\n",
    "print(\"Encoded Features Shape:\", encoded_features[\"feat\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CollisionNetPL:\n\tMissing key(s) in state_dict: \"model.point_cloud_encoder.SA_modules.0.mlps.0.0.weight\", \"model.point_cloud_encoder.SA_modules.0.mlps.0.0.bias\", \"model.point_cloud_encoder.SA_modules.0.mlps.0.2.weight\", \"model.point_cloud_encoder.SA_modules.0.mlps.0.2.bias\", \"model.point_cloud_encoder.SA_modules.0.mlps.0.4.weight\", \"model.point_cloud_encoder.SA_modules.0.mlps.0.4.bias\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.0.weight\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.0.bias\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.2.weight\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.2.bias\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.4.weight\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.4.bias\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.0.weight\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.0.bias\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.2.weight\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.2.bias\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.4.weight\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.4.bias\". \n\tUnexpected key(s) in state_dict: \"model.point_cloud_encoder.mlp.0.weight\", \"model.point_cloud_encoder.mlp.0.bias\", \"model.point_cloud_encoder.mlp.1.weight\", \"model.point_cloud_encoder.mlp.1.bias\", \"model.point_cloud_encoder.mlp.3.weight\", \"model.point_cloud_encoder.mlp.3.bias\", \"model.point_cloud_encoder.mlp.4.weight\", \"model.point_cloud_encoder.mlp.4.bias\", \"model.point_cloud_encoder.mlp.6.weight\", \"model.point_cloud_encoder.mlp.6.bias\", \"model.point_cloud_encoder.mlp.7.weight\", \"model.point_cloud_encoder.mlp.7.bias\", \"model.point_cloud_encoder.mlp.9.weight\", \"model.point_cloud_encoder.mlp.9.bias\", \"model.point_cloud_encoder.mlp.10.weight\", \"model.point_cloud_encoder.mlp.10.bias\", \"model.point_cloud_encoder.mlp.12.weight\", \"model.point_cloud_encoder.mlp.12.bias\", \"model.point_cloud_encoder.mlp.13.weight\", \"model.point_cloud_encoder.mlp.13.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints_pointnet/collisionnet-epoch=58-val_loss=0.14.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update with the actual checkpoint path\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCollisionNetPL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load the test dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _, test_loader \u001b[38;5;241m=\u001b[39m get_data_loaders_from_tensor(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./collision_bool/processed_test.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Update with the actual test dataset path\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     16\u001b[0m     train_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m  \u001b[38;5;66;03m# No training split, purely test data\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/pointnet2/lib/python3.8/site-packages/pytorch_lightning/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pointnet2/lib/python3.8/site-packages/pytorch_lightning/core/module.py:1582\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1501\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \n\u001b[1;32m   1581\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1582\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/mambaforge/envs/pointnet2/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m~/mambaforge/envs/pointnet2/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:187\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    184\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keys\u001b[38;5;241m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m~/mambaforge/envs/pointnet2/lib/python3.8/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CollisionNetPL:\n\tMissing key(s) in state_dict: \"model.point_cloud_encoder.SA_modules.0.mlps.0.0.weight\", \"model.point_cloud_encoder.SA_modules.0.mlps.0.0.bias\", \"model.point_cloud_encoder.SA_modules.0.mlps.0.2.weight\", \"model.point_cloud_encoder.SA_modules.0.mlps.0.2.bias\", \"model.point_cloud_encoder.SA_modules.0.mlps.0.4.weight\", \"model.point_cloud_encoder.SA_modules.0.mlps.0.4.bias\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.0.weight\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.0.bias\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.2.weight\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.2.bias\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.4.weight\", \"model.point_cloud_encoder.SA_modules.1.mlps.0.4.bias\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.0.weight\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.0.bias\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.2.weight\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.2.bias\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.4.weight\", \"model.point_cloud_encoder.SA_modules.2.mlps.0.4.bias\". \n\tUnexpected key(s) in state_dict: \"model.point_cloud_encoder.mlp.0.weight\", \"model.point_cloud_encoder.mlp.0.bias\", \"model.point_cloud_encoder.mlp.1.weight\", \"model.point_cloud_encoder.mlp.1.bias\", \"model.point_cloud_encoder.mlp.3.weight\", \"model.point_cloud_encoder.mlp.3.bias\", \"model.point_cloud_encoder.mlp.4.weight\", \"model.point_cloud_encoder.mlp.4.bias\", \"model.point_cloud_encoder.mlp.6.weight\", \"model.point_cloud_encoder.mlp.6.bias\", \"model.point_cloud_encoder.mlp.7.weight\", \"model.point_cloud_encoder.mlp.7.bias\", \"model.point_cloud_encoder.mlp.9.weight\", \"model.point_cloud_encoder.mlp.9.bias\", \"model.point_cloud_encoder.mlp.10.weight\", \"model.point_cloud_encoder.mlp.10.bias\", \"model.point_cloud_encoder.mlp.12.weight\", \"model.point_cloud_encoder.mlp.12.bias\", \"model.point_cloud_encoder.mlp.13.weight\", \"model.point_cloud_encoder.mlp.13.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from train import CollisionNetPL\n",
    "from data_loader import get_data_loaders_from_tensor\n",
    "\n",
    "# Path to the trained model checkpoint\n",
    "checkpoint_path = 'checkpoints_pointnet/collisionnet-epoch=58-val_loss=0.14.ckpt'  # Update with the actual checkpoint path\n",
    "\n",
    "# Load the trained model\n",
    "model = CollisionNetPL.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
    "\n",
    "# Load the test dataset\n",
    "_, test_loader = get_data_loaders_from_tensor(\n",
    "    \"./collision_bool/processed_test.pt\",  # Update with the actual test dataset path\n",
    "    batch_size=64,\n",
    "    train_ratio=0.01  # No training split, purely test data\n",
    ")\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "# Run the test\n",
    "results = trainer.test(model, dataloaders=test_loader)\n",
    "\n",
    "# Print the test results\n",
    "print(f\"Test results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CollisionNet(\n",
      "  (point_cloud_encoder): PointNet2(\n",
      "    (SA_modules): ModuleList(\n",
      "      (0): PointnetSAModule(\n",
      "        (groupers): ModuleList(\n",
      "          (0): QueryAndGroup()\n",
      "        )\n",
      "        (mlps): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): PointnetSAModule(\n",
      "        (groupers): ModuleList(\n",
      "          (0): QueryAndGroup()\n",
      "        )\n",
      "        (mlps): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(67, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (4): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PointnetSAModule(\n",
      "        (groupers): ModuleList(\n",
      "          (0): GroupAll()\n",
      "        )\n",
      "        (mlps): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(259, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (4): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc_layer): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (1): GroupNorm(16, 4096, eps=1e-05, affine=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (3): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "      (4): GroupNorm(16, 2048, eps=1e-05, affine=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (feature_encoder): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=32, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=2112, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.collisionnet import CollisionNet\n",
    "# Initialize the CollisionNet model\n",
    "collision_net = CollisionNet()\n",
    "\n",
    "# Print the structure of the CollisionNet model\n",
    "print(collision_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointnet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
